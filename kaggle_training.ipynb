{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Manipulation Tactic Detector - Training Notebook\n",
                "\n",
                "This notebook fine-tunes the `j-hartmann/emotion-english-distilroberta-base` model on the ManTacAi dataset to detect manipulation tactics.\n",
                "\n",
                "## Instructions\n",
                "1. Upload the `dataset_augmented/v5_training_data_final.json` file to your Kaggle environment as a dataset named `v5-training-data-final`.\n",
                "2. Run all cells to train the model.\n",
                "3. Download the `manipulation_tactic_detector_model.zip` file at the end."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies.\n",
                "# NOTE: You will likely see \"ERROR: pip's dependency resolver...\" regarding libraries like \n",
                "# 'cudf', 'bigframes', 'pyarrow', etc. \n",
                "# PLEASE IGNORE THESE ERRORS. They are for libraries we are NOT using.\n",
                "# As long as the check below passes, you are good to go.\n",
                "!pip install -q -U transformers datasets evaluate accelerate scikit-learn\n",
                "\n",
                "print(\"\\n--- Verifying Installation ---\")\n",
                "try:\n",
                "    import transformers\n",
                "    import datasets\n",
                "    import evaluate\n",
                "    import accelerate\n",
                "    import sklearn\n",
                "    print(\"\u2705 Success! All core dependencies imported correctly.\")\n",
                "    print(\"You can safely proceed to the next cells.\")\n",
                "except ImportError as e:\n",
                "    print(f\"\u274c Critical Error: {e}\")\n",
                "    raise e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "# Suppress tokenizers parallelism warning\n",
                "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
                "\n",
                "import json\n",
                "import numpy as np\n",
                "import evaluate\n",
                "from datasets import Dataset, DatasetDict\n",
                "from transformers import (\n",
                "    AutoTokenizer, \n",
                "    AutoModelForSequenceClassification, \n",
                "    TrainingArguments, \n",
                "    Trainer,\n",
                "    DataCollatorWithPadding\n",
                ")\n",
                "from sklearn.metrics import accuracy_score, f1_score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration & Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Label Mapping (Must match src/utils/config.py)\n",
                "id2label = {\n",
                "    0: \"ethical_persuasion\",\n",
                "    1: \"gaslighting\", \n",
                "    2: \"guilt_tripping\",\n",
                "    3: \"deflection\",\n",
                "    4: \"stonewalling\",\n",
                "    5: \"belittling_ridicule\",\n",
                "    6: \"love_bombing\",\n",
                "    7: \"threatening_intimidation\", \n",
                "    8: \"passive_aggression\",\n",
                "    9: \"appeal_to_emotion\",\n",
                "    10: \"whataboutism\",\n",
                "    11: \"neutral_conversation\",\n",
                "    12: \"coercive_control\"\n",
                "}\n",
                "label2id = {v: k for k, v in id2label.items()}\n",
                "\n",
                "# Load Dataset\n",
                "# Updated path for Kaggle input directory\n",
                "dataset_path = \"/kaggle/input/v5-training-data-final/v5_training_data_final.json\"\n",
                "\n",
                "try:\n",
                "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
                "        data = json.load(f)\n",
                "    print(f\"Successfully loaded dataset. Keys: {list(data.keys())}\")\n",
                "except FileNotFoundError:\n",
                "    print(f\"ERROR: File '{dataset_path}' not found. Please ensure the dataset is added to the notebook.\")\n",
                "    # Fallback for local testing if needed, or stop\n",
                "    raise\n",
                "\n",
                "def create_dataset_object(data_list):\n",
                "    # Filter out any items with unknown labels\n",
                "    valid_items = [\n",
                "        item for item in data_list \n",
                "        if item[\"manipulation_tactic\"] in label2id\n",
                "    ]\n",
                "    if len(valid_items) < len(data_list):\n",
                "        print(f\"Warning: Filtered out {len(data_list) - len(valid_items)} items with unknown labels.\")\n",
                "        \n",
                "    return Dataset.from_list([\n",
                "        {\n",
                "            \"text\": item[\"text\"],\n",
                "            \"label\": label2id[item[\"manipulation_tactic\"]]\n",
                "        } \n",
                "        for item in valid_items\n",
                "    ])\n",
                "\n",
                "# Handle different key names for validation split\n",
                "val_key = \"validation\" if \"validation\" in data else \"val\"\n",
                "\n",
                "if val_key in data and \"test\" in data:\n",
                "    raw_datasets = DatasetDict({\n",
                "        \"train\": create_dataset_object(data[\"train\"]),\n",
                "        \"validation\": create_dataset_object(data[val_key]),\n",
                "        \"test\": create_dataset_object(data[\"test\"])\n",
                "    })\n",
                "else:\n",
                "    print(\"Splitting 'train' set as validation/test sets were not found.\")\n",
                "    full_ds = create_dataset_object(data[\"train\"])\n",
                "    splits = full_ds.train_test_split(test_size=0.2, seed=42)\n",
                "    test_val = splits[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
                "    raw_datasets = DatasetDict({\n",
                "        \"train\": splits[\"train\"],\n",
                "        \"validation\": test_val[\"train\"],\n",
                "        \"test\": test_val[\"test\"]\n",
                "    })\n",
                "\n",
                "print(raw_datasets)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_checkpoint = \"j-hartmann/emotion-english-distilroberta-base\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
                "\n",
                "def preprocess_function(examples):\n",
                "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
                "\n",
                "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
                "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    model_checkpoint,\n",
                "    num_labels=len(id2label),\n",
                "    id2label=id2label,\n",
                "    label2id=label2id,\n",
                "    ignore_mismatched_sizes=True  # Essential for replacing the head\n",
                ")\n",
                "# Note: You will see a warning about weights not being initialized. \n",
                "# This is EXPECTED because we are replacing the emotion classification head with a new one."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(eval_pred):\n",
                "    predictions, labels = eval_pred\n",
                "    predictions = np.argmax(predictions, axis=1)\n",
                "    acc = accuracy_score(labels, predictions)\n",
                "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
                "    return {\"accuracy\": acc, \"f1\": f1}\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=16,\n",
                "    num_train_epochs=4,\n",
                "    weight_decay=0.01,\n",
                "    eval_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1\",\n",
                "    push_to_hub=False,\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_datasets[\"train\"],\n",
                "    eval_dataset=tokenized_datasets[\"validation\"],\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics,\n",
                ")\n",
                "\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluation & Saving"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "eval_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
                "print(f\"Test Results: {eval_results}\")\n",
                "\n",
                "# Save Model to Kaggle working directory\n",
                "save_path = \"/kaggle/working/manipulation_tactic_detector_model\"\n",
                "trainer.save_model(save_path)\n",
                "tokenizer.save_pretrained(save_path)\n",
                "\n",
                "# Zip for download\n",
                "!zip -r manipulation_tactic_detector_model.zip /kaggle/working/manipulation_tactic_detector_model"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}